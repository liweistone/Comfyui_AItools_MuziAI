import os
import comfy
import torch
import numpy as np
import logging
import time
import asyncio
from PIL import Image
import google.generativeai as genai
import requests
from io import BytesIO
import base64

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename=os.path.join(os.path.dirname(__file__), 'gemini_debug.log')
)

#===========Geminiè§†é¢‘æç¤ºç”Ÿæˆå™¨=============
class GeminiImageAnalyzer:
    """
    ğŸ¬ Geminiè§†é¢‘æç¤ºç”Ÿæˆå™¨ ğŸ¬
    åŠŸèƒ½ï¼šåˆ†æè§†é¢‘å¸§å›¾åƒå¹¶ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆæç¤ºè¯
    ç‰ˆæœ¬ï¼š2.2
    å¼€å‘è€…ï¼šæ‡‚AIçš„æœ¨å­
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE", {
                    "tooltip": "è¾“å…¥è§†é¢‘å¸§å›¾åƒï¼Œå°†ç”¨äºç”Ÿæˆè§†é¢‘æç¤ºè¯"
                }),
                "model_name": ([
                "gemini-1.5-flash", 
                "gemini-1.5-pro", 
                "gemini-1.5-flash-8b", 
                "gemini-2.0-flash-exp",
                "gemini-2.0-flash-lite", 
                "gemini-2.0-flash-exp-image-generation",
                "gemini-2.0-flash-thinking-exp-01-21", 
                "gemini-2.0-flash", 
                "gemini-2.0-pro", 
                "gemini-2.5-flash-preview-04-17"
                "gemini-2.5-pro-preview",
                "gemini-2.5-pro-preview-03-25", 
                "gemini-2.5-flash", 
                "gemini-2.5-pro", 
                ], {
                    "default": "gemini-1.5-pro",
                    "tooltip": "é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬"
                }),
                "max_tokens": ("INT", {
                    "default": 350, 
                    "min": 100, 
                    "max": 6000,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„é•¿åº¦"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7, 
                    "min": 0, 
                    "max": 1, 
                    "step": 0.05,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§(0=ç¡®å®šæ€§é«˜,1=åˆ›é€ æ€§é«˜)"
                }),
                "timeout": ("INT", {
                    "default": 30, 
                    "min": 30, 
                    "max": 300,
                    "tooltip": "APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)"
                }),
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "è¾“å…¥Gemini APIå¯†é’¥"
                }),
            },
            "optional": {
                "custom_prompt": ("STRING", {
                    "default": "è¯·ä»”ç»†åˆ†æè¿™å¼ è§†é¢‘å¸§å›¾ç‰‡ï¼Œä¸ºæˆ‘ç”Ÿæˆæ–‡ç”Ÿè§†é¢‘çš„æç¤ºè¯ï¼Œä½ è¦åšæœ€å¤§ç¨‹åº¦çš„è¯¦ç»†åˆ†æï¼Œè¯·ä½ è¯·æ ¹æ®ç”»é¢å†…å®¹ï¼Œä¸ºè§†é¢‘æ·»åŠ åˆé€‚çš„åŠ¨æ•ˆåŠè§†è§‰æ•ˆæœï¼Œä½ çš„åˆ†æè¦å®Œç¾çš„è¡¨ç°è§†é¢‘è´¨é‡åŠè§†é¢‘æ•ˆæœã€‚è¯·åŒæ—¶æä¾›ä¸­æ–‡å’Œè‹±æ–‡ç‰ˆæœ¬ï¼Œç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å†…å®¹)\n[ä¸­æ–‡ç»“æŸ]\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å†…å®¹)\n[è‹±æ–‡ç»“æŸ]", 
                    "multiline": True,
                    "tooltip": "è‡ªå®šä¹‰åˆ†ææŒ‡ä»¤"
                }),
                "proxy_url": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "ä»£ç†æœåŠ¡å™¨åœ°å€(å¦‚éœ€)"
                }),
                "help_text": ("STRING", {
                    "default": """ğŸ“Œ Geminiè§†é¢‘æç¤ºç”Ÿæˆå™¨ä½¿ç”¨æŒ‡å—

åŠŸèƒ½è¯´æ˜ï¼š
åˆ†æè§†é¢‘å¸§å›¾åƒå¹¶ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆæç¤ºè¯ï¼Œé€‚ç”¨äºæ–‡ç”Ÿè§†é¢‘å·¥ä½œæµã€‚

ğŸ”§ æ ¸å¿ƒå‚æ•°ï¼š
- image: è¾“å…¥è§†é¢‘å¸§å›¾åƒ
- model_name: é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬
- max_tokens: æ§åˆ¶è¾“å‡ºé•¿åº¦(100-6000)
- temperature: æ§åˆ¶åˆ›é€ æ€§(0-1)
- timeout: APIè¶…æ—¶æ—¶é—´(30-300ç§’)
- api_key: Gemini APIå¯†é’¥

ğŸ¯ å¯é€‰å‚æ•°:
- custom_prompt: è‡ªå®šä¹‰åˆ†ææŒ‡ä»¤
- proxy_url: ä»£ç†æœåŠ¡å™¨åœ°å€

ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
- è§†é¢‘å†…å®¹åˆ›ä½œ
- åŠ¨æ€æ•ˆæœè®¾è®¡
- è§†é¢‘åˆ†é•œè„šæœ¬ç”Ÿæˆ

ğŸ“ è¾“å‡ºè¯´æ˜ï¼š
- prompt_cn: ç”Ÿæˆçš„è§†é¢‘æç¤ºè¯(ä¸­æ–‡)
- prompt_en: ç”Ÿæˆçš„è§†é¢‘æç¤ºè¯(è‹±æ–‡)

æ›´å¤šAIå·¥å…·è¯·å…³æ³¨å…¬ä¼—å·: æ‡‚AIçš„æœ¨å­
                    """,
                    "multiline": True
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("STRING","STRING",)
    RETURN_NAMES = ("prompt_cn","prompt_en",)
    FUNCTION = "analyze_image"
    CATEGORY = "ğŸ¨å…¬ä¼—å·æ‡‚AIçš„æœ¨å­åšå·å·¥å…·/geminiæç¤ºè¯å¢å¼º"
    OUTPUT_NODE = True

    def __init__(self):
        self.initialized = False
        self.session = requests.Session()

    def initialize_model(self, api_key, proxy_url=""):
        if not self.initialized:
            genai.configure(
                api_key=api_key,
                transport="rest"
            )
            self.initialized = True
            logging.info("Geminiæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def preprocess_image(self, image):
        try:
            image_np = 255. * image[0].cpu().numpy()
            pil_image = Image.fromarray(np.clip(image_np, 0, 255).astype(np.uint8))
            
            max_size = 768
            if max(pil_image.size) > max_size:
                ratio = max_size / max(pil_image.size)
                new_size = (int(pil_image.width * ratio), int(pil_image.height * ratio))
                pil_image = pil_image.resize(new_size, Image.LANCZOS)
                logging.info(f"å›¾åƒå·²ä¼˜åŒ–è‡³ {new_size}")
            
            return pil_image
        except Exception as e:
            logging.error(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise

    def parse_response(self, text):
        """ä»å“åº”æ–‡æœ¬ä¸­æå–ä¸­è‹±æ–‡å†…å®¹"""
        try:
            cn_start = text.find("[ä¸­æ–‡å¼€å§‹]")
            cn_end = text.find("[ä¸­æ–‡ç»“æŸ]")
            en_start = text.find("[è‹±æ–‡å¼€å§‹]")
            en_end = text.find("[è‹±æ–‡ç»“æŸ]")
            
            if cn_start == -1 or cn_end == -1 or en_start == -1 or en_end == -1:
                return text, text  # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œè¿”å›ç›¸åŒå†…å®¹
            
            cn_content = text[cn_start+6:cn_end].strip()
            en_content = text[en_start+6:en_end].strip()
            
            return cn_content, en_content
        except Exception as e:
            logging.error(f"è§£æå“åº”å¤±è´¥: {str(e)}")
            return text, text

    def analyze_image(self, image, model_name, max_tokens, temperature, timeout, api_key, custom_prompt="", proxy_url="", help_text="", **kwargs):
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.initialize_model(api_key, proxy_url)
            
            # å›¾åƒé¢„å¤„ç†
            pil_image = self.preprocess_image(image)
            
            # å‡†å¤‡æç¤ºè¯
            prompt_parts = [custom_prompt if custom_prompt.strip() else "è¯·åˆ†ææ­¤è§†é¢‘å¸§å¹¶ç”Ÿæˆè§†é¢‘ç”Ÿæˆæç¤ºï¼ŒåŒæ—¶æä¾›ä¸­æ–‡å’Œè‹±æ–‡ç‰ˆæœ¬ï¼Œç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å†…å®¹)\n[ä¸­æ–‡ç»“æŸ]\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å†…å®¹)\n[è‹±æ–‡ç»“æŸ]", pil_image]
            
            # è°ƒç”¨API
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(
                prompt_parts,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                ),
                request_options={"timeout": timeout}
            )
            
            if not response.text:
                raise ValueError("APIè¿”å›ç©ºå“åº”")
                
            # è§£æå“åº”
            cn_content, en_content = self.parse_response(response.text)
            return (cn_content, en_content)
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            logging.error(error_msg, exc_info=True)
            return (error_msg, error_msg)


#===================Geminiå›¾ç‰‡æç¤ºç”Ÿæˆå™¨========================
class GeminiPromptGenerator:
    """
    ğŸ–¼ï¸ Geminiå›¾ç‰‡æç¤ºç”Ÿæˆå™¨ ğŸ–¼ï¸
    åŠŸèƒ½ï¼šåˆ†æå›¾åƒå¹¶ç”Ÿæˆé«˜è´¨é‡çš„æ–‡ç”Ÿå›¾æç¤ºè¯
    ç‰ˆæœ¬ï¼š2.2
    å¼€å‘è€…ï¼šæ‡‚AIçš„æœ¨å­
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE", {
                    "tooltip": "è¾“å…¥å›¾åƒï¼Œå°†ç”¨äºç”Ÿæˆæç¤ºè¯"
                }),
                "model_name": ([
                "gemini-1.5-flash", 
                "gemini-1.5-pro", 
                "gemini-1.5-flash-8b", 
                "gemini-2.0-flash-exp",
                "gemini-2.0-flash-lite", 
                "gemini-2.0-flash-exp-image-generation",
                "gemini-2.0-flash-thinking-exp-01-21", 
                "gemini-2.0-flash", 
                "gemini-2.0-pro", 
                "gemini-2.5-flash-preview-04-17"
                "gemini-2.5-pro-preview",
                "gemini-2.5-pro-preview-03-25", 
                "gemini-2.5-flash", 
                "gemini-2.5-pro", 
                ], {
                    "default": "gemini-1.5-flash", 
                    "tooltip": "é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬"
                }),
                "max_tokens": ("INT", {
                    "default": 1000, 
                    "min": 100, 
                    "max": 4000,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„é•¿åº¦"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7, 
                    "min": 0, 
                    "max": 1, 
                    "step": 0.05,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§(0=ç¡®å®šæ€§é«˜,1=åˆ›é€ æ€§é«˜)"
                }),
                "timeout": ("INT", {
                    "default": 30, 
                    "min": 30, 
                    "max": 300,
                    "tooltip": "APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)"
                }),
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "è¾“å…¥Gemini APIå¯†é’¥"
                }),
            },
            "optional": {
                "custom_prompt": ("STRING", {
                    "default": "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡å¹¶ä¸ºæˆ‘ç”Ÿæˆé«˜è´¨é‡çš„æ–‡ç”Ÿå›¾æç¤ºè¯ã€‚è¦æ±‚ï¼š1. è¯¦ç»†æè¿°ç”»é¢å†…å®¹ 2.åŒ…å«è‰ºæœ¯é£æ ¼ 3. æè¿°å…‰ç…§å’Œè‰²å½© 4. åŒ…å«ç”»é¢æ°›å›´ 5. è¯·åŒæ—¶æä¾›ä¸­æ–‡å’Œè‹±æ–‡ç‰ˆæœ¬ï¼Œç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å†…å®¹)\n[ä¸­æ–‡ç»“æŸ]\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å†…å®¹)\n[è‹±æ–‡ç»“æŸ]", 
                    "multiline": True,
                    "tooltip": "è‡ªå®šä¹‰åˆ†ææŒ‡ä»¤"
                }),
                "proxy_url": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "ä»£ç†æœåŠ¡å™¨åœ°å€(å¦‚éœ€)"
                }),
                "help_text": ("STRING", {
                    "default": """ğŸ“Œ Geminiå›¾ç‰‡æç¤ºç”Ÿæˆå™¨ä½¿ç”¨æŒ‡å—

åŠŸèƒ½è¯´æ˜ï¼š
åˆ†æå›¾åƒå¹¶ç”Ÿæˆé«˜è´¨é‡çš„æ–‡ç”Ÿå›¾æç¤ºè¯ï¼Œé€‚ç”¨äºAIç»˜ç”»å·¥ä½œæµã€‚

ğŸ”§ æ ¸å¿ƒå‚æ•°ï¼š
- image: è¾“å…¥å›¾åƒ
- model_name: é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬
- max_tokens: æ§åˆ¶è¾“å‡ºé•¿åº¦(100-4000)
- temperature: æ§åˆ¶åˆ›é€ æ€§(0-1)
- timeout: APIè¶…æ—¶æ—¶é—´(30-300ç§’)
- api_key: Gemini APIå¯†é’¥

ğŸ¯ å¯é€‰å‚æ•°:
- custom_prompt: è‡ªå®šä¹‰åˆ†ææŒ‡ä»¤
- proxy_url: ä»£ç†æœåŠ¡å™¨åœ°å€

ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
- AIç»˜ç”»æç¤ºè¯ç”Ÿæˆ
- å›¾åƒå†…å®¹åˆ†æ
- è‰ºæœ¯é£æ ¼è½¬æ¢

ğŸ“ è¾“å‡ºè¯´æ˜ï¼š
- prompt_en: ç”Ÿæˆçš„å›¾åƒæç¤ºè¯(è‹±æ–‡)
- prompt_cn: ç”Ÿæˆçš„å›¾åƒæç¤ºè¯(ä¸­æ–‡)

æ›´å¤šAIå·¥å…·è¯·å…³æ³¨å…¬ä¼—å·: æ‡‚AIçš„æœ¨å­
                    """,
                    "multiline": True
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("STRING","STRING",)
    RETURN_NAMES = ("prompt_en","prompt_cn",)
    FUNCTION = "generate_prompt"
    CATEGORY = "ğŸ¨å…¬ä¼—å·æ‡‚AIçš„æœ¨å­åšå·å·¥å…·/geminiæç¤ºè¯å¢å¼º"
    OUTPUT_NODE = True

    def __init__(self):
        self.initialized = False
        self.session = requests.Session()

    def initialize_model(self, api_key, proxy_url=""):
        if not self.initialized:
            genai.configure(
                api_key=api_key,
                transport="rest"
            )
            self.initialized = True
            logging.info("Geminiæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def preprocess_image(self, image):
        try:
            image_np = 255. * image[0].cpu().numpy()
            pil_image = Image.fromarray(np.clip(image_np, 0, 255).astype(np.uint8))
            
            max_size = 1024
            if max(pil_image.size) > max_size:
                ratio = max_size / max(pil_image.size)
                new_size = (int(pil_image.width * ratio), int(pil_image.height * ratio))
                pil_image = pil_image.resize(new_size, Image.LANCZOS)
                logging.info(f"å›¾åƒå·²ä¼˜åŒ–è‡³ {new_size}")
            
            return pil_image
        except Exception as e:
            logging.error(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise

    def parse_response(self, text):
        """ä»å“åº”æ–‡æœ¬ä¸­æå–ä¸­è‹±æ–‡å†…å®¹"""
        try:
            cn_start = text.find("[ä¸­æ–‡å¼€å§‹]")
            cn_end = text.find("[ä¸­æ–‡ç»“æŸ]")
            en_start = text.find("[è‹±æ–‡å¼€å§‹]")
            en_end = text.find("[è‹±æ–‡ç»“æŸ]")
            
            if cn_start == -1 or cn_end == -1 or en_start == -1 or en_end == -1:
                return text, text  # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œè¿”å›ç›¸åŒå†…å®¹
            
            en_content = text[en_start+6:en_end].strip()
            cn_content = text[cn_start+6:cn_end].strip()
            
            return en_content, cn_content
        except Exception as e:
            logging.error(f"è§£æå“åº”å¤±è´¥: {str(e)}")
            return text, text

    def generate_prompt(self, image, model_name, max_tokens, temperature, timeout, api_key, custom_prompt="", proxy_url="", help_text="", **kwargs):
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.initialize_model(api_key, proxy_url)
            
            # å›¾åƒé¢„å¤„ç†
            pil_image = self.preprocess_image(image)
            
            # å‡†å¤‡æç¤ºè¯
            prompt_parts = [custom_prompt if custom_prompt.strip() else "è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡å¹¶ç”Ÿæˆé«˜è´¨é‡çš„æ–‡ç”Ÿå›¾æç¤ºè¯ï¼ŒåŒæ—¶æä¾›ä¸­æ–‡å’Œè‹±æ–‡ç‰ˆæœ¬ï¼Œç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å†…å®¹)\n[ä¸­æ–‡ç»“æŸ]\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å†…å®¹)\n[è‹±æ–‡ç»“æŸ]", pil_image]
            
            # è°ƒç”¨API
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(
                prompt_parts,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                ),
                request_options={"timeout": timeout}
            )
            
            if not response.text:
                raise ValueError("APIè¿”å›ç©ºå“åº”")
                
            # è§£æå“åº”
            en_content, cn_content = self.parse_response(response.text)
            return (en_content, cn_content)
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            logging.error(error_msg, exc_info=True)
            return (error_msg, error_msg)


# ====================æç¤ºè¯ä¼˜åŒ–èŠ‚ç‚¹ ===========
class GeminiPromptOptimizer:
    """
    âœ¨ Geminiæç¤ºè¯ä¼˜åŒ–å™¨ âœ¨
    åŠŸèƒ½ï¼šä¼˜åŒ–å’Œå¢å¼ºAIç»˜ç”»æç¤ºè¯
    ç‰ˆæœ¬ï¼š2.2
    å¼€å‘è€…ï¼šæ‡‚AIçš„æœ¨å­
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input_prompt": ("STRING", {
                    "default": "", 
                    "multiline": True,
                    "tooltip": "è¾“å…¥éœ€è¦ä¼˜åŒ–çš„æç¤ºè¯"
                }),
                "model_name": ([
                "gemini-1.5-flash", 
                "gemini-1.5-pro", 
                "gemini-1.5-flash-8b", 
                "gemini-2.0-flash-exp",
                "gemini-2.0-flash-lite", 
                "gemini-2.0-flash-exp-image-generation",
                "gemini-2.0-flash-thinking-exp-01-21", 
                "gemini-2.0-flash", 
                "gemini-2.0-pro", 
                "gemini-2.5-flash-preview-04-17"
                "gemini-2.5-pro-preview",
                "gemini-2.5-pro-preview-03-25", 
                "gemini-2.5-flash", 
                "gemini-2.5-pro", 
                ], {
                    "default": "gemini-1.5-flash",
                    "tooltip": "é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬"
                }),
                "max_tokens": ("INT", {
                    "default": 1000, 
                    "min": 100, 
                    "max": 4000,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„é•¿åº¦"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7, 
                    "min": 0, 
                    "max": 1, 
                    "step": 0.05,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§(0=ç¡®å®šæ€§é«˜,1=åˆ›é€ æ€§é«˜)"
                }),
                "timeout": ("INT", {
                    "default": 30, 
                    "min": 30, 
                    "max": 300,
                    "tooltip": "APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)"
                }),
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "è¾“å…¥Gemini APIå¯†é’¥"
                }),
                "optimization_level": (["basic", "detailed", "professional"], {
                    "default": "detailed",
                    "tooltip": "é€‰æ‹©ä¼˜åŒ–çº§åˆ«"
                }),
            },
            "optional": {
                "custom_instruction": ("STRING", {
                    "default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„FLUXæç¤ºè¯å·¥ç¨‹å¸ˆï¼Œè¯·ä¼˜åŒ–ä»¥ä¸‹æç¤ºè¯ã€‚è¦æ±‚ï¼š1.æ·»åŠ åˆç†çš„è‰ºæœ¯åŒ–ç»†èŠ‚ 2.ä¿æŒé•¿åº¦åœ¨400å­—ç¬¦ä»¥ä¸Š 3.é¿å…è´Ÿé¢æè¿° 4.è¯·åŒæ—¶æä¾›è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬ï¼Œç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å†…å®¹)\n[è‹±æ–‡ç»“æŸ]\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å†…å®¹)\n[ä¸­æ–‡ç»“æŸ]", 
                    "multiline": True,
                    "tooltip": "è‡ªå®šä¹‰ä¼˜åŒ–æŒ‡ä»¤"
                }),
                "proxy_url": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "ä»£ç†æœåŠ¡å™¨åœ°å€(å¦‚éœ€)"
                }),
                "help_text": ("STRING", {
                    "default": """ğŸ“Œ Geminiæç¤ºè¯ä¼˜åŒ–å™¨ä½¿ç”¨æŒ‡å—

åŠŸèƒ½è¯´æ˜ï¼š
ä¼˜åŒ–å’Œå¢å¼ºAIç»˜ç”»æç¤ºè¯ï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚

ğŸ”§ æ ¸å¿ƒå‚æ•°ï¼š
- input_prompt: è¾“å…¥éœ€è¦ä¼˜åŒ–çš„æç¤ºè¯
- model_name: é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬
- max_tokens: æ§åˆ¶è¾“å‡ºé•¿åº¦(100-4000)
- temperature: æ§åˆ¶åˆ›é€ æ€§(0-1)
- timeout: APIè¶…æ—¶æ—¶é—´(30-300ç§’)
- api_key: Gemini APIå¯†é’¥
- optimization_level: ä¼˜åŒ–çº§åˆ«(basic/detailed/professional)

ğŸ¯ å¯é€‰å‚æ•°:
- custom_instruction: è‡ªå®šä¹‰ä¼˜åŒ–æŒ‡ä»¤
- proxy_url: ä»£ç†æœåŠ¡å™¨åœ°å€

ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
- æå‡AIç»˜ç”»æç¤ºè¯è´¨é‡
- æ·»åŠ è‰ºæœ¯åŒ–ç»†èŠ‚
- ä¸“ä¸šçº§å•†ä¸šç”¨é€”ä¼˜åŒ–

ğŸ“ è¾“å‡ºè¯´æ˜ï¼š
- optimized_prompt_en: ä¼˜åŒ–åçš„æç¤ºè¯(è‹±æ–‡)
- optimized_prompt_cn: ä¼˜åŒ–åçš„æç¤ºè¯(ä¸­æ–‡)

æ›´å¤šAIå·¥å…·è¯·å…³æ³¨å…¬ä¼—å·: æ‡‚AIçš„æœ¨å­
                    """,
                    "multiline": True
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("STRING","STRING",)
    RETURN_NAMES = ("optimized_prompt_en","optimized_prompt_cn",)
    FUNCTION = "optimize_prompt"
    CATEGORY = "ğŸ¨å…¬ä¼—å·æ‡‚AIçš„æœ¨å­åšå·å·¥å…·/geminiæç¤ºè¯å¢å¼º"
    OUTPUT_NODE = True

    def __init__(self):
        self.initialized = False
        self.session = requests.Session()

    def initialize_model(self, api_key, proxy_url=""):
        if not self.initialized:
            genai.configure(
                api_key=api_key,
                transport="rest"
            )
            self.initialized = True
            logging.info("Geminiæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def parse_response(self, text):
        """ä»å“åº”æ–‡æœ¬ä¸­æå–ä¸­è‹±æ–‡å†…å®¹"""
        try:
            cn_start = text.find("[ä¸­æ–‡å¼€å§‹]")
            cn_end = text.find("[ä¸­æ–‡ç»“æŸ]")
            en_start = text.find("[è‹±æ–‡å¼€å§‹]")
            en_end = text.find("[è‹±æ–‡ç»“æŸ]")
            
            if cn_start == -1 or cn_end == -1 or en_start == -1 or en_end == -1:
                return text, text  # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œè¿”å›ç›¸åŒå†…å®¹
            
            en_content = text[en_start+6:en_end].strip()
            cn_content = text[cn_start+6:cn_end].strip()
            
            return en_content, cn_content
        except Exception as e:
            logging.error(f"è§£æå“åº”å¤±è´¥: {str(e)}")
            return text, text

    def optimize_prompt(self, input_prompt, model_name, max_tokens, temperature, timeout, api_key, optimization_level="detailed", custom_instruction="", proxy_url="", help_text="", **kwargs):
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.initialize_model(api_key, proxy_url)
            
            # æ ¹æ®ä¼˜åŒ–çº§åˆ«è°ƒæ•´æç¤º
            level_instructions = {
                "basic": "è¯·å¯¹ä»¥ä¸‹æç¤ºè¯è¿›è¡ŒåŸºç¡€ä¼˜åŒ–ï¼Œä¸»è¦æ”¹å–„è¯­æ³•å’Œæ¸…æ™°åº¦ã€‚è¯·åŒæ—¶æä¾›è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬ï¼Œç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å†…å®¹)\n[è‹±æ–‡ç»“æŸ]\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å†…å®¹)\n[ä¸­æ–‡ç»“æŸ]",
                "detailed": "è¯·è¯¦ç»†ä¼˜åŒ–ä»¥ä¸‹æç¤ºè¯ï¼Œå¢åŠ æ›´å¤šç»†èŠ‚æè¿°ï¼ŒåŒæ—¶ä¿æŒåŸæ„ã€‚è¯·åŒæ—¶æä¾›è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬ï¼Œç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å†…å®¹)\n[è‹±æ–‡ç»“æŸ]\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å†…å®¹)\n[ä¸­æ–‡ç»“æŸ]",
                "professional": "è¯·ä¸“ä¸šçº§ä¼˜åŒ–ä»¥ä¸‹æç¤ºè¯ï¼Œä½¿å…¶é€‚åˆå•†ä¸šç”¨é€”ï¼ŒåŒ…å«ä¸°å¯Œçš„ä¸“ä¸šæœ¯è¯­å’Œç²¾ç¡®æè¿°ã€‚è¯·åŒæ—¶æä¾›è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬ï¼Œç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å†…å®¹)\n[è‹±æ–‡ç»“æŸ]\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å†…å®¹)\n[ä¸­æ–‡ç»“æŸ]"
            }
            
            base_instruction = custom_instruction if custom_instruction.strip() else level_instructions.get(optimization_level, "")
            
            # å‡†å¤‡å®Œæ•´æç¤º
            full_prompt = f"{base_instruction}\n\nåŸå§‹æç¤ºè¯ï¼š{input_prompt}"
            
            # è°ƒç”¨API
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                ),
                request_options={"timeout": timeout}
            )
            
            if not response.text:
                raise ValueError("APIè¿”å›ç©ºå“åº”")
                
            # è§£æå“åº”
            en_content, cn_content = self.parse_response(response.text)
            return (en_content, cn_content)
            
        except Exception as e:
            error_msg = f"æç¤ºè¯ä¼˜åŒ–å¤±è´¥: {str(e)}"
            logging.error(error_msg, exc_info=True)
            return (error_msg, error_msg)


#======================GeminièŠå¤©å™¨=======================
class GeminiChatPro:
    """
    ğŸ’¬ GeminièŠå¤©å™¨ ğŸ’¬
    åŠŸèƒ½ï¼šä¸Geminiæ¨¡å‹è¿›è¡Œå¯¹è¯äº¤äº’
    ç‰ˆæœ¬ï¼š2.2
    å¼€å‘è€…ï¼šæ‡‚AIçš„æœ¨å­
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "è¾“å…¥ä½ æƒ³è¯¢é—®çš„å†…å®¹"
                }),
                "model_name": ([
                "gemini-1.5-flash", 
                "gemini-1.5-pro", 
                "gemini-1.5-flash-8b", 
                "gemini-2.0-flash-exp",
                "gemini-2.0-flash-lite", 
                "gemini-2.0-flash-exp-image-generation",
                "gemini-2.0-flash-thinking-exp-01-21", 
                "gemini-2.0-flash", 
                "gemini-2.0-pro", 
                "gemini-2.5-flash-preview-04-17"
                "gemini-2.5-pro-preview",
                "gemini-2.5-pro-preview-03-25", 
                "gemini-2.5-flash", 
                "gemini-2.5-pro", 
                ], {
                    "default": "gemini-1.5-pro",
                    "tooltip": "é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬"
                }),
                "max_tokens": ("INT", {
                    "default": 1000, 
                    "min": 100, 
                    "max": 4000,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„é•¿åº¦"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7, 
                    "min": 0, 
                    "max": 1, 
                    "step": 0.05,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§(0=ç¡®å®šæ€§é«˜,1=åˆ›é€ æ€§é«˜)"
                }),
                "timeout": ("INT", {
                    "default": 30, 
                    "min": 30, 
                    "max": 300,
                    "tooltip": "APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)"
                }),
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "è¾“å…¥Gemini APIå¯†é’¥"
                }),
            },
            "optional": {
                "custom_instruction": ("STRING", {
                    "default": "è¯·ç”¨ä¸­æ–‡å’Œè‹±æ–‡ä¸¤ç§è¯­è¨€å›ç­”æˆ‘çš„é—®é¢˜ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å›ç­”)\n[ä¸­æ–‡ç»“æŸ]\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å›ç­”)\n[è‹±æ–‡ç»“æŸ]", 
                    "multiline": True,
                    "tooltip": "è‡ªå®šä¹‰å¯¹è¯æŒ‡ä»¤"
                }),
                "proxy_url": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "ä»£ç†æœåŠ¡å™¨åœ°å€(å¦‚éœ€)"
                }),
                "help_text": ("STRING", {
                    "default": """ğŸ“Œ GeminièŠå¤©å™¨ä½¿ç”¨æŒ‡å—

åŠŸèƒ½è¯´æ˜ï¼š
ä¸Geminiæ¨¡å‹è¿›è¡Œå¯¹è¯äº¤äº’ï¼Œå¯ç”¨äºå¤šç§AIç›¸å…³ä»»åŠ¡ã€‚

ğŸ”§ æ ¸å¿ƒå‚æ•°ï¼š
- input_prompt: è¾“å…¥ä½ æƒ³è¯¢é—®çš„å†…å®¹
- model_name: é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬
- max_tokens: æ§åˆ¶è¾“å‡ºé•¿åº¦(100-4000)
- temperature: æ§åˆ¶åˆ›é€ æ€§(0-1)
- timeout: APIè¶…æ—¶æ—¶é—´(30-300ç§’)
- api_key: Gemini APIå¯†é’¥

ğŸ¯ å¯é€‰å‚æ•°:
- custom_instruction: è‡ªå®šä¹‰å¯¹è¯æŒ‡ä»¤
- proxy_url: ä»£ç†æœåŠ¡å™¨åœ°å€

ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
- AIç›¸å…³é—®é¢˜å’¨è¯¢
- åˆ›æ„å†…å®¹ç”Ÿæˆ
- æŠ€æœ¯é—®é¢˜è§£ç­”
- æç¤ºè¯ä¼˜åŒ–å»ºè®®

ğŸ“ è¾“å‡ºè¯´æ˜ï¼š
- response_cn: Geminiæ¨¡å‹çš„å›å¤å†…å®¹(ä¸­æ–‡)
- response_en: Geminiæ¨¡å‹çš„å›å¤å†…å®¹(è‹±æ–‡)

æ›´å¤šAIå·¥å…·è¯·å…³æ³¨å…¬ä¼—å·: æ‡‚AIçš„æœ¨å­
                    """,
                    "multiline": True
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("STRING","STRING",)
    RETURN_NAMES = ("response_cn","response_en",)
    FUNCTION = "chat_with_gemini"
    CATEGORY = "ğŸ¨å…¬ä¼—å·æ‡‚AIçš„æœ¨å­åšå·å·¥å…·/geminiæç¤ºè¯å¢å¼º"
    OUTPUT_NODE = True

    def __init__(self):
        self.initialized = False
        self.session = requests.Session()

    def initialize_model(self, api_key, proxy_url=""):
        if not self.initialized:
            genai.configure(
                api_key=api_key,
                transport="rest"
            )
            self.initialized = True
            logging.info("Geminiæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def parse_response(self, text):
        """ä»å“åº”æ–‡æœ¬ä¸­æå–ä¸­è‹±æ–‡å†…å®¹"""
        try:
            cn_start = text.find("[ä¸­æ–‡å¼€å§‹]")
            cn_end = text.find("[ä¸­æ–‡ç»“æŸ]")
            en_start = text.find("[è‹±æ–‡å¼€å§‹]")
            en_end = text.find("[è‹±æ–‡ç»“æŸ]")
            
            if cn_start == -1 or cn_end == -1 or en_start == -1 or en_end == -1:
                return text, text  # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œè¿”å›ç›¸åŒå†…å®¹
            
            cn_content = text[cn_start+6:cn_end].strip()
            en_content = text[en_start+6:en_end].strip()
            
            return cn_content, en_content
        except Exception as e:
            logging.error(f"è§£æå“åº”å¤±è´¥: {str(e)}")
            return text, text

    def chat_with_gemini(self, input_prompt, model_name, max_tokens, temperature, timeout, api_key, custom_instruction="", proxy_url="", help_text="", **kwargs):
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.initialize_model(api_key, proxy_url)
            
            # å‡†å¤‡å®Œæ•´æç¤º
            full_prompt = f"{custom_instruction}\n\nç”¨æˆ·è¾“å…¥ï¼š{input_prompt}" if custom_instruction.strip() else f"è¯·ç”¨ä¸­æ–‡å’Œè‹±æ–‡ä¸¤ç§è¯­è¨€å›ç­”æˆ‘çš„é—®é¢˜ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n\n[ä¸­æ–‡å¼€å§‹]\n(ä¸­æ–‡å›ç­”)\n[ä¸­æ–‡ç»“æŸ]\n\n[è‹±æ–‡å¼€å§‹]\n(è‹±æ–‡å›ç­”)\n[è‹±æ–‡ç»“æŸ]\n\næˆ‘çš„é—®é¢˜æ˜¯ï¼š{input_prompt}"
            
            # è°ƒç”¨API
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                ),
                request_options={"timeout": timeout}
            )
            
            if not response.text:
                raise ValueError("APIè¿”å›ç©ºå“åº”")
                
            # è§£æå“åº”
            cn_content, en_content = self.parse_response(response.text)
            return (cn_content, en_content)
            
        except Exception as e:
            error_msg = f"å¯¹è¯å¤±è´¥: {str(e)}"
            logging.error(error_msg, exc_info=True)
            return (error_msg, error_msg)


NODE_CLASS_MAPPINGS = {
    "GeminiImageAnalyzer": GeminiImageAnalyzer,
    "GeminiPromptGenerator": GeminiPromptGenerator,
    "GeminiPromptOptimizer": GeminiPromptOptimizer,
    "GeminiChatPro": GeminiChatPro
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiImageAnalyzer": "ğŸ¬ Geminiè§†é¢‘æç¤ºç”Ÿæˆå™¨",
    "GeminiPromptGenerator": "ğŸ–¼ï¸ Geminiå›¾ç‰‡æç¤ºç”Ÿæˆå™¨",
    "GeminiPromptOptimizer": "âœ¨ Geminiæç¤ºè¯ä¼˜åŒ–å™¨",
    "GeminiChatPro": "ğŸ’¬ GeminièŠå¤©å™¨"
}