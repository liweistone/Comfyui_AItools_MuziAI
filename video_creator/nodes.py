import os
import json
import logging
from pathlib import Path
import google.generativeai as genai
import torch
import numpy as np
from PIL import Image
import random
import time
from comfy.sd import CLIP
from comfy.cli_args import LatentPreviewMethod
import folder_paths
import comfy.utils
import comfy.sample
import comfy.samplers
import comfy.sd
import comfy.model_management
from comfy_extras.nodes_clip_sdxl import CLIPTextEncodeSDXL
from nodes import CLIPTextEncode
from comfy.model_patcher import ModelPatcher

# é…ç½®æ—¥å¿— - ä¿®æ­£è·¯å¾„æ‹¼æ¥
log_dir = os.path.join(os.path.dirname(__file__), 'logs')
os.makedirs(log_dir, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename=os.path.join(log_dir, 'gemini_video_creator.log')
)

class ConfigManager:
    @classmethod
    def get_templates(cls):
        """æ›´å¥å£®çš„æ¨¡æ¿åŠ è½½æ–¹å¼"""
        try:
            template_dir = os.path.join(os.path.dirname(__file__), 'templates')
            templates = {}
            for file in Path(template_dir).glob('*.json'):
                with open(file, 'r', encoding='utf-8') as f:
                    templates[file.stem] = json.load(f)
            return templates
        except Exception as e:
            logging.error(f"åŠ è½½æ¨¡æ¿å¤±è´¥: {str(e)}")
            return {
                "drama": {"instruction": "é»˜è®¤å‰§æœ¬æ¨¡æ¿"},
                "comedy": {"instruction": "é»˜è®¤å–œå‰§æ¨¡æ¿"},
                "documentary": {"instruction": "é»˜è®¤çºªå½•ç‰‡æ¨¡æ¿"},
                "poetry": {"instruction": "é»˜è®¤å¤è¯—æ¨¡æ¿"},
                "classical_poetry": {"instruction": "é«˜åº¦è¿˜åŸå¤è¯—è¯æ¨¡æ¿"},
            }

TEMPLATES = ConfigManager.get_templates()

#==============GeminiçŸ­è§†é¢‘åˆ›ä½œå™¨=============
class GeminiVideoCreator:
    """
    ğŸ¬ GeminiçŸ­è§†é¢‘åˆ›ä½œå™¨ ğŸ¬
    åŠŸèƒ½ï¼š
    - æ ¹æ®åˆ›æ„è‡ªåŠ¨ç”ŸæˆçŸ­è§†é¢‘å‰§æœ¬
    - è‡ªåŠ¨ç”Ÿæˆåˆ†é•œæè¿°å’ŒAIç»˜ç”»æç¤ºè¯
    - æ”¯æŒå¤šç§è§†é¢‘ç±»å‹å’Œé£æ ¼
    å¾®ä¿¡ï¼šstone_liwei
    ç‰ˆæœ¬ï¼š2.1ï¼ˆæ–°å¢ä½¿ç”¨è¯´æ˜å’Œå‚æ•°è¯´æ˜ï¼‰
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "idea": ("STRING", {
                    "default": "", 
                    "multiline": True,
                    "tooltip": "è¾“å…¥ä½ çš„è§†é¢‘åˆ›æ„æˆ–ä¸»é¢˜æè¿°"
                }),
                "genre": (list(TEMPLATES.keys()), {
                    "default": "drama",
                    "tooltip": "é€‰æ‹©è§†é¢‘ç±»å‹/é£æ ¼æ¨¡æ¿"
                }),
                "duration": (["30s", "1m", "3m", "5m"], {
                    "default": "1m",
                    "tooltip": "é€‰æ‹©è§†é¢‘æ—¶é•¿"
                }),
                "model_name": ([
                "gemini-1.5-flash", 
                "gemini-1.5-pro", 
                "gemini-1.5-flash-8b", 
                "gemini-2.0-flash-exp",
                "gemini-2.0-flash-lite", 
                "gemini-2.0-flash-exp-image-generation",
                "gemini-2.0-flash-thinking-exp-01-21", 
                "gemini-2.0-flash", 
                "gemini-2.0-pro", 
                "gemini-2.5-flash-preview-04-17"
                "gemini-2.5-pro-preview",
                "gemini-2.5-pro-preview-03-25", 
                "gemini-2.5-flash", 
                "gemini-2.5-pro", 
                ], {
                    "default": "gemini-1.5-flash",
                    "tooltip": "é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬"
                }),
                "max_tokens": ("INT", {
                    "default": 1000, 
                    "min": 500, 
                    "max": 6000,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„é•¿åº¦"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7, 
                    "min": 0, 
                    "max": 1, 
                    "step": 0.05,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§(0=ç¡®å®šæ€§é«˜,1=åˆ›é€ æ€§é«˜)"
                }),
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "è¾“å…¥Gemini APIå¯†é’¥(å¯ä»Google AI Studioè·å–)"
                }),
            },
            "optional": {
                "custom_instruction": ("STRING", {
                    "default": "è¯·æ ¹æ®ä»¥ä¸‹åˆ›æ„ç”Ÿæˆä¸€ä¸ªçŸ­è§†é¢‘å‰§æœ¬ï¼Œè¦æ±‚ï¼š1.åŒ…å«å®Œæ•´æ•…äº‹ç»“æ„ 2.æœ‰æ˜ç¡®çš„å¼€å¤´ã€å‘å±•å’Œç»“å°¾ 3.é€‚åˆçŸ­è§†é¢‘å¹³å°ä¼ æ’­ 4.åŒ…å«3-5ä¸ªä¸»è¦åœºæ™¯ 5.ä¸è¦ä½¿ç”¨Markdownæ ¼å¼",
                    "multiline": True,
                    "tooltip": "è‡ªå®šä¹‰ç”ŸæˆæŒ‡ä»¤ï¼Œè¦†ç›–é»˜è®¤æ¨¡æ¿"
                }),
                "proxy_url": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "ä»£ç†æœåŠ¡å™¨åœ°å€(å¦‚éœ€)"
                }),
                "style_reference": ("IMAGE", {
                    "tooltip": "ä¸Šä¼ å‚è€ƒå›¾åƒä»¥ä¿æŒé£æ ¼ä¸€è‡´"
                }),
                "help_text": ("STRING", {
                    "default": """ğŸ“œ GeminiçŸ­è§†é¢‘åˆ›ä½œå™¨ä½¿ç”¨è¯´æ˜

åŠŸèƒ½è¯´æ˜ï¼šæ ¹æ®åˆ›æ„è‡ªåŠ¨ç”ŸæˆçŸ­è§†é¢‘å‰§æœ¬ã€åˆ†é•œæè¿°å’ŒAIç»˜ç”»æç¤ºè¯

ğŸ”§ å‚æ•°è¯´æ˜ï¼š
idea: è¾“å…¥ä½ çš„è§†é¢‘åˆ›æ„æˆ–ä¸»é¢˜æè¿°
genre: é€‰æ‹©è§†é¢‘ç±»å‹/é£æ ¼æ¨¡æ¿
duration: é€‰æ‹©è§†é¢‘æ—¶é•¿
model_name: é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬
max_tokens: æ§åˆ¶ç”Ÿæˆå†…å®¹çš„é•¿åº¦
temperature: æ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§(0=ç¡®å®šæ€§é«˜,1=åˆ›é€ æ€§é«˜)
api_key: Gemini APIå¯†é’¥(å¯ä»Google AI Studioè·å–)

ğŸ¯ å¯é€‰å‚æ•°:
custom_instruction: è‡ªå®šä¹‰ç”ŸæˆæŒ‡ä»¤
proxy_url: ä»£ç†æœåŠ¡å™¨åœ°å€(å¦‚éœ€)
style_reference: ä¸Šä¼ å‚è€ƒå›¾åƒä»¥ä¿æŒé£æ ¼ä¸€è‡´

ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
- çŸ­è§†é¢‘å†…å®¹åˆ›ä½œ
- çŸ­å‰§å‰§æœ¬ç”Ÿæˆ
- åˆ†é•œè„šæœ¬è‡ªåŠ¨ç”Ÿæˆ
- AIç»˜ç”»æç¤ºè¯æ‰¹é‡ç”Ÿæˆ

ğŸ“Œ è¾“å‡ºè¯´æ˜ï¼š
1. å‰§æœ¬: å®Œæ•´çš„æ•…äº‹å‰§æœ¬
2. åˆ†é•œå†…å®¹: è¯¦ç»†çš„é•œå¤´æè¿°
3. åˆ†é•œæç¤ºè¯: å¯ç”¨äºAIç»˜ç”»çš„æç¤ºè¯

æ›´å¤šAIå·¥å…·è¯·å…³æ³¨å…¬ä¼—å·: æ‡‚AIçš„æœ¨å­
                    """,
                    "multiline": True
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("å‰§æœ¬", "åˆ†é•œå†…å®¹", "åˆ†é•œæç¤ºè¯")
    FUNCTION = "generate_video_plan"
    CATEGORY = "ğŸ¨å…¬ä¼—å·æ‡‚AIçš„æœ¨å­åšå·å·¥å…·/geminiè§†é¢‘åˆ›ä½œçŸ­è§†é¢‘çŸ­å‰§ç‰ˆ"

    def __init__(self):
        self.initialized = False
        self.session = None

    def initialize_model(self, api_key, proxy_url=None):
        """æ›´å¥å£®çš„æ¨¡å‹åˆå§‹åŒ–"""
        if not self.initialized:
            try:
                config = {
                    "api_key": api_key or os.getenv("GOOGLE_API_KEY"),
                    "transport": "rest"
                }
                if proxy_url:
                    config["client_options"] = {
                        "api_endpoint": proxy_url
                    }
                
                genai.configure(**config)
                self.initialized = True
                logging.info("Geminiæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logging.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                raise

    def generate_video_plan(self, idea, genre, duration, model_name, max_tokens, temperature, api_key, custom_instruction="", proxy_url="", style_reference=None, help_text="", unique_id=None, **kwargs):
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.initialize_model(api_key, proxy_url if proxy_url else None)
            
            # å‡†å¤‡æç¤ºè¯
            template = TEMPLATES.get(genre, TEMPLATES["drama"])
            prompt = f"""è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›å†…å®¹ï¼ˆä¸è¦é—æ¼ä»»ä½•éƒ¨åˆ†ï¼‰ï¼š
            
            === å‰§æœ¬ ===
            {custom_instruction if custom_instruction else template['instruction']}
            åˆ›æ„: {idea}
            æ—¶é•¿: {duration}
            
            === åˆ†é•œ ===
            æŒ‰é¡ºåºæè¿°æ¯ä¸ªé•œå¤´ï¼ŒåŒ…å«:
            1. é•œå¤´ç¼–å·
            2. é•œå¤´ç±»å‹ï¼ˆå…¨æ™¯/ä¸­æ™¯/ç‰¹å†™ï¼‰
            3. ç”»é¢å†…å®¹
            4. æ—¶é•¿ä¼°ç®—
            
            === æç¤ºè¯ ===
            ä¸ºæ¯ä¸ªé•œå¤´ç”ŸæˆAIç»˜ç”»æç¤ºè¯ï¼Œè¦æ±‚:
            1. é£æ ¼ä¸€è‡´
            2. åŒ…å«é•œå¤´ç±»å‹æè¿°
            3. ä½¿ç”¨ä¸­æ–‡
            """
            
            # å¤„ç†å›¾åƒè¾“å…¥
            image_part = None
            if style_reference is not None:
                image_part = self.process_image(style_reference)
            
            # è°ƒç”¨API
            model = genai.GenerativeModel(model_name)
            if image_part:
                response = model.generate_content([prompt, image_part],
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=max_tokens,
                        temperature=temperature
                    )
                )
            else:
                response = model.generate_content(prompt,
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=max_tokens,
                        temperature=temperature
                    )
                )
            
            if not response.text:
                raise ValueError("APIè¿”å›ç©ºå“åº”")
                
            # æ›´å¥å£®çš„å“åº”è§£æ
            return self.parse_response(response.text)
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
            logging.error(error_msg, exc_info=True)
            return (error_msg, error_msg, error_msg)

    def process_image(self, image_tensor):
        """æ›´å®‰å…¨çš„å›¾åƒå¤„ç†"""
        try:
            image_np = 255. * image_tensor[0].cpu().numpy()
            pil_image = Image.fromarray(np.clip(image_np, 0, 255).astype(np.uint8))
            
            # é™åˆ¶æœ€å¤§å°ºå¯¸
            max_size = 1024
            if max(pil_image.size) > max_size:
                ratio = max_size / max(pil_image.size)
                new_size = (int(pil_image.width * ratio), int(pil_image.height * ratio))
                pil_image = pil_image.resize(new_size, Image.LANCZOS)
            
            return pil_image
        except Exception as e:
            logging.error(f"å›¾åƒå¤„ç†å¤±è´¥: {str(e)}")
            return None

    def parse_response(self, text):
        """æ”¹è¿›çš„å“åº”è§£æå™¨"""
        try:
            result = {
                "script": "",
                "storyboard": "",
                "prompts": ""
            }
            
            # æ›´ç²¾ç¡®çš„æ®µè½åˆ†å‰²
            sections = [s.strip() for s in text.split("===") if s.strip()]
            for i in range(0, len(sections)-1, 2):
                section_type = sections[i].lower()
                content = sections[i+1]
                
                if "å‰§æœ¬" in section_type:
                    result["script"] = content
                elif "åˆ†é•œ" in section_type or "storyboard" in section_type:
                    result["storyboard"] = content
                elif "æç¤º" in section_type or "prompt" in section_type:
                    result["prompts"] = content
            
            # å›é€€æœºåˆ¶ï¼šå¦‚æœæœªæŒ‰æ ¼å¼è¿”å›ï¼Œå°è¯•æ™ºèƒ½åˆ†å‰²
            if not all(result.values()):
                parts = text.split("\n\n")
                if len(parts) >= 3:
                    result = {
                        "script": parts[0],
                        "storyboard": parts[1],
                        "prompts": "\n".join(parts[2:])
                    }
            
            return (result["script"], result["storyboard"], result["prompts"])
        except Exception as e:
            logging.error(f"è§£æå“åº”å¤±è´¥: {str(e)}")
            return (text, text, text)

#===============åˆ†é•œè½¬æç¤ºè¯ä¼˜åŒ–å™¨==================
class GeminiStoryboardToPrompts:
    """
    ğŸ“¹ åˆ†é•œè½¬æç¤ºè¯ä¼˜åŒ–å™¨ ğŸ“¹
    åŠŸèƒ½ï¼š
    - å°†åˆ†é•œæè¿°è½¬æ¢ä¸ºé«˜è´¨é‡çš„AIç»˜ç”»æç¤ºè¯
    - æ”¯æŒå¤šç§è‰ºæœ¯é£æ ¼
    - ä¿æŒå¤šé•œå¤´é—´é£æ ¼ä¸€è‡´æ€§
    å¾®ä¿¡ï¼šstone_liwei
    ç‰ˆæœ¬ï¼š2.1ï¼ˆæ–°å¢ä½¿ç”¨è¯´æ˜å’Œå‚æ•°è¯´æ˜ï¼‰
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "storyboard": ("STRING", {
                    "default": "", 
                    "multiline": True,
                    "tooltip": "è¾“å…¥åˆ†é•œæè¿°æ–‡æœ¬"
                }),
                "model_name": ([
                "gemini-1.5-flash", 
                "gemini-1.5-pro", 
                "gemini-1.5-flash-8b", 
                "gemini-2.0-flash-exp",
                "gemini-2.0-flash-lite", 
                "gemini-2.0-flash-exp-image-generation",
                "gemini-2.0-flash-thinking-exp-01-21", 
                "gemini-2.0-flash", 
                "gemini-2.0-pro", 
                "gemini-2.5-flash-preview-04-17"
                "gemini-2.5-pro-preview",
                "gemini-2.5-pro-preview-03-25", 
                "gemini-2.5-flash", 
                "gemini-2.5-pro", 
                ], {
                    "default": "gemini-1.5-flash",
                    "tooltip": "é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬"
                }),
                "style": (["realistic", "anime", "cinematic", "watercolor", "3d_render"], {
                    "default": "cinematic",
                    "tooltip": "é€‰æ‹©è‰ºæœ¯é£æ ¼"
                }),
                "max_tokens": ("INT", {
                    "default": 1500, 
                    "min": 500, 
                    "max": 4000,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„é•¿åº¦"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.6, 
                    "min": 0, 
                    "max": 1, 
                    "step": 0.05,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§(0=ç¡®å®šæ€§é«˜,1=åˆ›é€ æ€§é«˜)"
                }),
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "è¾“å…¥Gemini APIå¯†é’¥(å¯ä»Google AI Studioè·å–)"
                }),
                "timeout": ("INT", {
                    "default": 45, 
                    "min": 10, 
                    "max": 180,
                    "tooltip": "APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)"
                }),
            },
            "optional": {
                "custom_instruction": ("STRING", {
                    "default": "è¯·æ ¹æ®ä»¥ä¸‹åˆ†é•œæè¿°ç”ŸæˆAIç»˜ç”»æç¤ºè¯ï¼Œè¦æ±‚ï¼š1.ä¿æŒé£æ ¼ä¸€è‡´ 2.åŒ…å«åœºæ™¯ç»†èŠ‚ 3.ä½¿ç”¨è‹±æ–‡ 4.æ¯ä¸ªæç¤ºè¯ä¸å°‘äº100ä¸ªå­—ç¬¦",
                    "multiline": True,
                    "tooltip": "è‡ªå®šä¹‰ç”ŸæˆæŒ‡ä»¤"
                }),
                "proxy_url": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "tooltip": "ä»£ç†æœåŠ¡å™¨åœ°å€(å¦‚éœ€)"
                }),
                "help_text": ("STRING", {
                    "default": """ğŸ“œ åˆ†é•œè½¬æç¤ºè¯ä¼˜åŒ–å™¨ä½¿ç”¨è¯´æ˜

åŠŸèƒ½è¯´æ˜ï¼šå°†åˆ†é•œæè¿°è½¬æ¢ä¸ºé«˜è´¨é‡çš„AIç»˜ç”»æç¤ºè¯

ğŸ”§ å‚æ•°è¯´æ˜ï¼š
storyboard: è¾“å…¥åˆ†é•œæè¿°æ–‡æœ¬
model_name: é€‰æ‹©Geminiæ¨¡å‹ç‰ˆæœ¬
style: é€‰æ‹©è‰ºæœ¯é£æ ¼
max_tokens: æ§åˆ¶ç”Ÿæˆå†…å®¹çš„é•¿åº¦
temperature: æ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§
api_key: Gemini APIå¯†é’¥
timeout: APIè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)

ğŸ¯ å¯é€‰å‚æ•°:
custom_instruction: è‡ªå®šä¹‰ç”ŸæˆæŒ‡ä»¤
proxy_url: ä»£ç†æœåŠ¡å™¨åœ°å€(å¦‚éœ€)

ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
- å°†åˆ†é•œè„šæœ¬è½¬æ¢ä¸ºAIç»˜ç”»æç¤ºè¯
- æ‰¹é‡ç”Ÿæˆé£æ ¼ä¸€è‡´çš„æç¤ºè¯
- ä¸ºåŠ¨ç”»åˆ¶ä½œå‡†å¤‡ç´ æ

ğŸ“Œ è¾“å‡ºè¯´æ˜ï¼š
1. prompts: å¯ç”¨äºAIç»˜ç”»çš„è¯¦ç»†æç¤ºè¯(è‹±æ–‡)

æ›´å¤šAIå·¥å…·è¯·å…³æ³¨å…¬ä¼—å·: æ‡‚AIçš„æœ¨å­
                    """,
                    "multiline": True
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("prompts",)
    FUNCTION = "generate_prompts"
    CATEGORY = "ğŸ¨å…¬ä¼—å·æ‡‚AIçš„æœ¨å­åšå·å·¥å…·/geminiè§†é¢‘åˆ›ä½œçŸ­è§†é¢‘çŸ­å‰§ç‰ˆ"

    def __init__(self):
        self.initialized = False
        self.max_retries = 3  # æ–°å¢é‡è¯•æœºåˆ¶

    def initialize_model(self, api_key, proxy_url=None):
        """æ”¹è¿›çš„æ¨¡å‹åˆå§‹åŒ–"""
        if not self.initialized:
            try:
                config = {
                    "api_key": api_key or os.getenv("GOOGLE_API_KEY"),
                    "transport": "rest"
                }
                if proxy_url:
                    config["client_options"] = {
                        "api_endpoint": proxy_url
                    }
                
                genai.configure(**config)
                self.initialized = True
                logging.info("Geminiæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logging.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.initialized = False
                raise

    def generate_prompts(self, storyboard, model_name, style, max_tokens, temperature, api_key, timeout, custom_instruction="", proxy_url="", help_text="", unique_id=None, **kwargs):
        try:
            # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            for attempt in range(self.max_retries):
                try:
                    self.initialize_model(api_key, proxy_url if proxy_url else None)
                    break
                except Exception as e:
                    if attempt == self.max_retries - 1:
                        raise
                    logging.warning(f"åˆå§‹åŒ–å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt+1}/{self.max_retries})")
                    time.sleep(2 ** attempt)
            
            # æ„å»ºæç¤ºè¯æ¨¡æ¿
            prompt_template = f"""
            {custom_instruction if custom_instruction else f"è¯·ç”Ÿæˆ{style}é£æ ¼çš„AIç»˜ç”»æç¤ºè¯"}
            
            åˆ†é•œå†…å®¹:
            {storyboard}
            
            ç”Ÿæˆè¦æ±‚:
            1. æ¯ä¸ªåˆ†é•œå¯¹åº”ä¸€ä¸ªæç¤ºè¯
            2. ä¿æŒè§†è§‰é£æ ¼ä¸€è‡´æ€§
            3. åŒ…å«é•œå¤´è§’åº¦æè¿°
            4. ä½¿ç”¨è‹±æ–‡
            5. æ ¼å¼ç¤ºä¾‹: [Scene 1]: detailed prompt here
            """
            
            logging.info(f"å¼€å§‹ç”Ÿæˆæç¤ºè¯ï¼Œè¶…æ—¶è®¾ç½®: {timeout}s")
            
            # è°ƒç”¨APIï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(
                prompt_template,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                ),
                request_options={"timeout": timeout}
            )
            
            # éªŒè¯å“åº”
            if not response.text:
                raise ValueError("APIè¿”å›ç©ºå“åº”")
            if len(response.text) < 50:
                raise ValueError("å“åº”å†…å®¹è¿‡çŸ­")
                
            logging.info("æç¤ºè¯ç”ŸæˆæˆåŠŸ")
            return (response.text.strip(),)
            
        except Exception as e:
            error_msg = f"æç¤ºè¯ç”Ÿæˆå¤±è´¥: {str(e)}"
            logging.error(error_msg, exc_info=True)
            return (f"ERROR: {error_msg}",)

#===========æç¤ºè¯æ‰¹æ¬¡é€‰æ‹©å™¨ (æœ¨å­AI)=======================
class PromptSequencer:
    """
    âœ¨ æœ¨å­AIæç¤ºè¯åºåˆ—å™¨ âœ¨
    åŠŸèƒ½ï¼š
    - ä»å¤šè¡Œæ–‡æœ¬ä¸­é€‰æ‹©æŒ‡å®šè¡Œï¼ˆ1-basedï¼‰
    - æ”¯æŒé¡ºåº/éšæœºä¸¤ç§è¯»å–æ¨¡å¼
    å¾®ä¿¡ï¼šstone_liwei
    ç‰ˆæœ¬ï¼š2.1ï¼ˆæ–°å¢ä½¿ç”¨è¯´æ˜å’Œå‚æ•°è¯´æ˜ï¼‰
    """
    def __init__(self):
        self.prompts = []
        self.current_index = 0  # ç”¨äºé¡ºåºæ¨¡å¼
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt_list": ("STRING", {
                    "multiline": True, 
                    "default": "ç¬¬ä¸€è¡Œæç¤ºè¯\nç¬¬äºŒè¡Œæç¤ºè¯\nç¬¬ä¸‰è¡Œæç¤ºè¯",
                    "dynamicPrompts": False,
                    "tooltip": "è¾“å…¥å¤šè¡Œæç¤ºè¯ï¼Œæ¯è¡Œä¸€ä¸ª"
                }),
                "mode": (["sequential", "random", "manual"], {
                    "default": "sequential",
                    "tooltip": "sequential:é¡ºåºè¯»å–\nrandom:éšæœºé€‰æ‹©\nmanual:æ‰‹åŠ¨æŒ‡å®šè¡Œå·"
                }),
                "seed": ("INT", {
                    "default": 0, 
                    "min": 0, 
                    "max": 0xffffffffffffffff,
                    "tooltip": "éšæœºæ¨¡å¼ä¸‹ä½¿ç”¨çš„ç§å­å€¼\n0è¡¨ç¤ºæ¯æ¬¡éšæœº"
                }),
            },
            "optional": {
                "line_number": ("INT", {
                    "default": 1, 
                    "min": 1, 
                    "max": 9999,
                    "step": 1,
                    "display": "number",
                    "tooltip": "æ‰‹åŠ¨æ¨¡å¼ä¸‹æŒ‡å®šè¦é€‰æ‹©çš„è¡Œå·"
                }),
                "reset_counter": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "é‡ç½®é¡ºåºæ¨¡å¼çš„è®¡æ•°å™¨"
                }),
                "help_text": ("STRING", {
                    "default": """ğŸ“œ æç¤ºè¯æ‰¹æ¬¡é€‰æ‹©å™¨ä½¿ç”¨è¯´æ˜

åŠŸèƒ½è¯´æ˜ï¼šä»å¤šè¡Œæç¤ºè¯ä¸­é€‰æ‹©æŒ‡å®šè¡Œ/è®©æ‰¹é‡ç”Ÿå›¾å˜å¾—æ›´ç®€å•ï¼Œæ”¯æŒé¡ºåº/éšæœº/æ‰‹åŠ¨ä¸‰ç§æ¨¡å¼

ğŸ”§ å‚æ•°è¯´æ˜ï¼š
prompt_list: è¾“å…¥å¤šè¡Œæç¤ºè¯ï¼Œæ¯è¡Œä¸€ä¸ª
mode:
sequential - é¡ºåºè¯»å–(è‡ªåŠ¨å¾ªç¯)
random - éšæœºé€‰æ‹©
manual - æ‰‹åŠ¨æŒ‡å®šè¡Œå·
seed: éšæœºæ¨¡å¼ç§å­å€¼(0=çœŸéšæœº)
line_number: æ‰‹åŠ¨æ¨¡å¼ä¸‹çš„è¡Œå·
reset_counter: é‡ç½®é¡ºåºæ¨¡å¼çš„è®¡æ•°å™¨

ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
æ‰¹é‡ç”Ÿæˆå¤šå¼ å›¾ç‰‡æ—¶è‡ªåŠ¨åˆ‡æ¢æç¤ºè¯
A/Bæµ‹è¯•ä¸åŒæç¤ºè¯æ•ˆæœ
åˆ¶ä½œåŠ¨ç”»æ—¶æŒ‰é¡ºåºåˆ‡æ¢åœºæ™¯
æ›´å¤šAIå·¥å…·è¯·å…³æ³¨å…¬ä¼—å·: æ‡‚AIçš„æœ¨å­
                    """,
                    "multiline": True
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }
    
    RETURN_TYPES = ("STRING", "INT")
    RETURN_NAMES = ("prompt", "è¡Œå·")
    FUNCTION = "get_selected_prompt"
    CATEGORY = "ğŸ¨å…¬ä¼—å·æ‡‚AIçš„æœ¨å­åšå·å·¥å…·/geminiè§†é¢‘åˆ›ä½œçŸ­è§†é¢‘çŸ­å‰§ç‰ˆ"
    
    def get_selected_prompt(self, prompt_list, mode, seed=0, line_number=1, reset_counter=False, help_text="", unique_id=None):
        try:
            # å¤„ç†æ–‡æœ¬è¾“å…¥
            self.prompts = [line.strip() for line in prompt_list.split('\n') if line.strip()]
            
            if not self.prompts:
                return ("[é”™è¯¯] æ²¡æœ‰æœ‰æ•ˆçš„æç¤ºè¯å†…å®¹", 0)
            
            # é‡ç½®è®¡æ•°å™¨
            if reset_counter:
                self.current_index = 0
            
            selected_index = 0
            line_num = 0
            
            if mode == "manual":
                # æ‰‹åŠ¨æŒ‡å®šæ¨¡å¼
                selected_index = max(0, min(line_number - 1, len(self.prompts) - 1))
                line_num = line_number
            elif mode == "random":
                # éšæœºæ¨¡å¼
                random.seed(seed if seed != 0 else None)
                selected_index = random.randint(0, len(self.prompts) - 1)
                line_num = selected_index + 1
            else:
                # é¡ºåºæ¨¡å¼
                if self.current_index >= len(self.prompts):
                    self.current_index = 0  # å¾ªç¯è¯»å–
                
                selected_index = self.current_index
                line_num = self.current_index + 1
                self.current_index += 1
            
            return (self.prompts[selected_index], line_num)
            
        except Exception as e:
            return (f"[é”™è¯¯] å‘ç”Ÿå¼‚å¸¸: {str(e)}", 0)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "GeminiVideoCreator": GeminiVideoCreator,
    "GeminiStoryboardToPrompts": GeminiStoryboardToPrompts,
    "PromptSequencer": PromptSequencer
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiVideoCreator": "ğŸ¬çŸ­å‰§çŸ­è§†é¢‘åˆ›ä½œå™¨",
    "GeminiStoryboardToPrompts": "ğŸ“¹åˆ†é•œè½¬æç¤ºè¯ä¼˜åŒ–å™¨",
    "PromptSequencer": "ğŸ“œæç¤ºè¯æ‰¹æ¬¡é€‰æ‹©å™¨ (æ‡‚AIçš„æœ¨å­)"
}